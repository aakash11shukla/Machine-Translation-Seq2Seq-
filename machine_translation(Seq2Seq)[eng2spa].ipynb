{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, Dense, Input, LSTM\n",
    "from keras.models import Model, load_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "if len(K.tensorflow_backend._get_available_gpus()) > 0:\n",
    "    from keras.layers import CuDNNLSTM as LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "EPOCHS = 100\n",
    "EMBEDDING_DIM = 100\n",
    "MAX_VOCAB_SIZE = 20000\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "NUM_SAMPLES = 30000\n",
    "LATENT_DIM = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = []\n",
    "target_texts = []\n",
    "target_input_texts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "with open('D:/Downloads/spa-eng/spa.txt', encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        count+=1\n",
    "        if count > NUM_SAMPLES:\n",
    "            break\n",
    "        if '\\t' not in line:\n",
    "            continue\n",
    "        input_text, translation = line.rstrip().split('\\t')\n",
    "        \n",
    "        input_texts.append(input_text)\n",
    "        target_input_texts.append('<sos> '+translation)\n",
    "        target_texts.append(translation+' <eos>')\n",
    "    print('Number of samples %d' % len(input_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_inputs = Tokenizer(num_words=MAX_VOCAB_SIZE)\n",
    "tokenizer_inputs.fit_on_texts(input_texts)\n",
    "input_sequences = tokenizer_inputs.texts_to_sequences(input_texts)\n",
    "\n",
    "word2idx_inputs = tokenizer_inputs.word_index\n",
    "print('Num of input words %d' % len(word2idx_inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len_input = max(len(s) for s in input_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_outputs = Tokenizer(num_words=MAX_VOCAB_SIZE, filters='')\n",
    "tokenizer_outputs.fit_on_texts(target_texts + target_input_texts)\n",
    "target_sequences = tokenizer_outputs.texts_to_sequences(target_texts)\n",
    "target_input_sequences = tokenizer_outputs.texts_to_sequences(target_input_texts)\n",
    "\n",
    "word2idx_outputs = tokenizer_outputs.word_index\n",
    "print('Num of output words %d' % len(word2idx_outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words_output = len(word2idx_outputs) + 1\n",
    "max_len_target = max(len(s) for s in target_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = pad_sequences(input_sequences, maxlen=max_len_input)\n",
    "print('encoder_inputs shape: ', encoder_inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = pad_sequences(target_input_sequences, maxlen=max_len_target, padding='post')\n",
    "print('decoder_inputs shape: ', decoder_inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_targets = pad_sequences(target_sequences, maxlen=max_len_target, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = {}\n",
    "with open('D:/Downloads/glove.6B/glove.6B.%sd.txt' % EMBEDDING_DIM, encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vec = np.asarray(values[1:], dtype='float32')\n",
    "        word2vec[word] = vec\n",
    "    print('Num of word vectors %d' % len(word2vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Filling pre-trained embeddings..')\n",
    "num_words = max(MAX_VOCAB_SIZE, len(word2idx_inputs)+1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word2idx_inputs.items():\n",
    "    if i < MAX_VOCAB_SIZE:\n",
    "        embedding_vector = word2vec.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(\n",
    "    num_words,\n",
    "    EMBEDDING_DIM,\n",
    "    weights=[embedding_matrix],\n",
    "    input_length=max_len_input,\n",
    "    trainable=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_targets_one_hot = np.zeros((len(input_texts), max_len_target, num_words_output), dtype='float32')\n",
    "\n",
    "for i, d in enumerate(decoder_targets):\n",
    "    for t, word in enumerate(d):\n",
    "        decoder_targets_one_hot[i, t, word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs_placeholder = Input(shape=(max_len_input,))\n",
    "x = embedding_layer(encoder_inputs_placeholder)\n",
    "encoder = LSTM(LATENT_DIM, return_state=True)\n",
    "encoder_outputs, h, c = encoder(x)\n",
    "encoder_states = [h, c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs_placeholder = Input(shape=(max_len_target,))\n",
    "decoder_embedding = Embedding(num_words_output, LATENT_DIM, trainable=True)\n",
    "decoder_inputs_x = decoder_embedding(decoder_inputs_placeholder)\n",
    "decoder_lstm = LSTM(LATENT_DIM, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs_x, initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_words_output, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(filepath):\n",
    "    print(\"loading model\")\n",
    "    model = load_model(filepath)\n",
    "else:\n",
    "    model = Model([encoder_inputs_placeholder, decoder_inputs_placeholder], decoder_outputs)\n",
    "    model.compile(\n",
    "        optimizer='rmsprop',\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "filepath = \"eng2spa.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=0, save_best_only=True, mode='min', period=10)\n",
    "callbacks_list = [checkpoint]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "r = model.fit(\n",
    "    [encoder_inputs, decoder_inputs],\n",
    "    decoder_targets_one_hot,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_split=0.2,\n",
    "    callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(r.history['loss'], label='loss')\n",
    "plt.plot(r.history['valid_loss'], label='valid_loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(r.history('acc'), label='accuracy')\n",
    "plt.plot(r.history('valid_acc'), label='valid_accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs_placeholder, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input((LATENT_DIM,))\n",
    "decoder_state_input_c = Input((LATENT_DIM,))\n",
    "decoder_states_input = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_input_single = Input((1,))\n",
    "decoder_input_single_x = decoder_embedding(decoder_input_single)\n",
    "decoder_outputs, h, c = decoder_lstm(decoder_input_single_x, initial_state=decoder_states_input)\n",
    "decoder_states = [h, c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_model = Model([decoder_input_single, decoder_states_input], [decoder_outputs]+decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word_eng = {v:k for k,v in word2idx_inputs.items()}\n",
    "idx2word_trans = {v:k for k,v in word2idx_outputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    \n",
    "    states_values = encoder_model.predict(input_seq)\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    target_seq[0, 0] = word2idx_outputs['<sos>']\n",
    "    \n",
    "    eos = word2idx_outputs['<eos>']\n",
    "    \n",
    "    output_sentence = []\n",
    "    for _ in range(max_len_target):\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_values)\n",
    "        \n",
    "        idx = np.argmax(output_tokens[0, 0, :])\n",
    "        \n",
    "        if eos == idx:\n",
    "            break\n",
    "        \n",
    "        word = ''\n",
    "        if idx > 0:\n",
    "            word = idx2word_trans[idx]\n",
    "            output_sentence.append(word)\n",
    "        \n",
    "        target_seq[0, 0]=idx\n",
    "        states_value = [h, c]\n",
    "    \n",
    "    return ' '.join(output_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    i = np.random.choice(len(input_texts))\n",
    "    input_seq = encoder_inputs[i:i+1]\n",
    "    translation = decode_sequence(input_seq)\n",
    "    print('---')\n",
    "    print('Input: '+input_texts[i])\n",
    "    print('Translation: '+translation)\n",
    "    \n",
    "    ans = input(\"Continue [Y/n]?\")\n",
    "    if ans and ans.lower().startswith('n'):\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
